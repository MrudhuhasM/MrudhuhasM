# Hi, I'm Mrudhuhas ğŸ‘‹

**ML Engineer | Training â†’ Inference â†’ Production**

8+ years building ML systems end-to-end: from training custom models to optimizing inference at the GPU kernel level.

## What I Do

ğŸ”¥ **Inference Optimization** â€” Custom Triton kernels (Flash Attention, LayerNorm, GELU) achieving 1.85x speedup  
ğŸ¤– **LLM Applications** â€” Production RAG/Agents with LangGraph, 60-70% cost reduction  
ğŸ§  **Model Training** â€” Fine-tuning transformers (BERT, T5, Llama) with LoRA/QLoRA  
â˜ï¸ **Production** â€” GCP/AWS deployments serving millions of requests  

## Featured Projects

### [Triton-GPT2](https://github.com/MrudhuhasM/triton-gpt2) â€” GPU Kernel Development
GPT-2 inference engine with custom Triton kernels. **275 TPS vs 149 TPS HuggingFace (1.85x speedup)**
- Fused Flash Attention matching PyTorch SDPA
- Custom LayerNorm, GELU, Softmax kernels
- KV-cache for autoregressive decoding

### [Meditations RAG](https://github.com/MrudhuhasM/meditations-rag) â€” Production LLM Application  
Agentic RAG with LangGraph: Controller â†’ Retriever â†’ Generator â†’ Evaluator loop  
[**Live Demo**](https://meditations-rag-180347172582.asia-south1.run.app) | Sub-500ms latency | 500+ RPS

## Tech Stack

**GPU/Inference**: Triton, CUDA, vLLM, TGI, Quantization (GPTQ, AWQ)  
**LLM Apps**: LangGraph, LangChain, LlamaIndex, RAG  
**Training**: PyTorch, LoRA/QLoRA, Mixed Precision, DeepSpeed  
**Production**: GCP, Docker, Kubernetes, FastAPI, Redis  

---

ğŸ“« **Open to remote opportunities (contract or full-time) | Flexible on US/EU timezones**

[LinkedIn](https://linkedin.com/in/mrudhuhas) â€¢ [Email](mailto:mrudhuhas@gmail.com)
